{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "PCA_entropy_analysis.ipynb",
   "provenance": [],
   "mount_file_id": "1ftglsv10oJsBwVJsNG1N4JccO_E9eVjO",
   "authorship_tag": "ABX9TyOxWs5PN2EttzhIMHdSyWKV"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AC7futGIBaaP"
   },
   "source": [
    "### このノートはMEM後のデータに対して, PCAとentropy計算による解析を行うためのものです. \n",
    "\n",
    "このノートに関しては, 基本は最初の行でgoogle driveをマウントして, あとはパスだけ入れればすべての行を実行で解析できるはずです.  \n",
    "## (注意) 現状, Igor上でimchi3_dataを2次元に直して (最新版Data_preprocessing.ipfが必要), csv書き出ししないと解析できません. この点に関しても将来的には解消する予定 (それよりもmatlab実装したほうがよいでしょうか？) \n",
    "\n",
    "---\n",
    "#### To do \n",
    "1. .ibwそのまま読み込み\n",
    "2. クラスタ数の自動決定 (G-means?)\n",
    "3. z stackデータへの対応\n",
    "4. PCAのみの解析\n",
    "\n",
    "PCA 参考ページ https://qiita.com/NoriakiOshita/items/460247bb57c22973a5f0  \n",
    "k-means 参考ページ https://aiacademy.jp/media/?p=254  \n",
    "Google colab参考ページ　https://pyhoo.jp/google-colaboratory-manual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVtIY8wHY-XR"
   },
   "source": [
    "\n",
    "### 最初にgoogle driveをマウントします　　\n",
    "カーソルを下の文字列のどこかにおいて, shift + enterを押してください.  \n",
    "\n",
    "リンクをクリックするとログインを求められるので, そちらで表示される文字列をコピーしてください. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YhSGsXGj3GSs",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1635943542568,
     "user_tz": -540,
     "elapsed": 352,
     "user": {
      "displayName": "加納研究室",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgEguPo0XY7dL8EbjanoKwJ9rFrRjru00p40AXk_Q=s64",
      "userId": "04223418218813721824"
     }
    },
    "outputId": "b02cb2d4-23a4-42a3-f4d0-8f5dfd95177c"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-b80391d2b2cf>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[0mgoogle\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolab\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mdrive\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mdrive\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmount\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'/content/drive'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'google'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HD5EDrSZ08Pp"
   },
   "source": [
    "基本的には最初の行に, 必要事項を入力して, あとはshift + Enterで進めていけば解析が可能です. \n",
    "\n",
    "---\n",
    "### パス文字列の取得法を以下に示します"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSx7BwPYbFiO"
   },
   "source": [
    "1. マウントが終わったら左のファイルボタンをクリックします."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMGmyvEh4hEJ"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7PV4VZ7b-Qy"
   },
   "source": [
    "3. 解析したいファイルが見つかったら, そのファイルの名前の右側の〇ポチを押してパスをコピーというところをクリックします. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p97iX_8jcN0j"
   },
   "source": [
    "### この時点で, データパスが取得できているので, これ以降の行でパスを入れるところに入れてください. 横軸についても同様にしてください. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24EX9KgcccdS"
   },
   "source": [
    "あとは基本的にはすべて実行で大丈夫です.  \n",
    "ctrl+F9でもいいですし, このページの上のほうにある\"ランタイム/すべてのセルを実行\"でもいいですし, shift + Enter連打でも大丈夫です.  \n",
    "\n",
    "\n",
    "最後の行に少し時間がかかります. 101*401のデータで2分くらいでしょうか.  \n",
    "終わったら, google drive内に結果フォルダを作ってくれていますので, そちらからダウンロードしていただければと思います. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrvEgEQldSoa"
   },
   "source": [
    "# パスとその他のパラメータを入れてください"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MYEkLs7T1Vj0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1635943542569,
     "user_tz": -540,
     "elapsed": 5,
     "user": {
      "displayName": "加納研究室",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgEguPo0XY7dL8EbjanoKwJ9rFrRjru00p40AXk_Q=s64",
      "userId": "04223418218813721824"
     }
    }
   },
   "source": [
    "# データのパス\n",
    "data_path = \"C:/Users/Shinichi/Desktop/Murakami/imchi3_2d.csv\"\n",
    "axis_path = \"C:/Users/Shinichi/Desktop/Murakami/re_ramanshift2.csv\"\n",
    "# PCA後のデータをいくつのクラスターに分けるか\n",
    "cluster_num = 15\n",
    "# データの縦幅\n",
    "x = 681\n",
    "# データの横幅\n",
    "y = 501\n",
    "# 詳細に解析する波数範囲 (指紋領域用にしています)\n",
    "subrange = (600,1800)"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9YvZv1hg0RHw",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1635943543962,
     "user_tz": -540,
     "elapsed": 1397,
     "user": {
      "displayName": "加納研究室",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgEguPo0XY7dL8EbjanoKwJ9rFrRjru00p40AXk_Q=s64",
      "userId": "04223418218813721824"
     }
    }
   },
   "source": [
    "# moduleのインポート\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.cluster import KMeans"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qWbM13RY07kt",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1635943544308,
     "user_tz": -540,
     "elapsed": 348,
     "user": {
      "displayName": "加納研究室",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgEguPo0XY7dL8EbjanoKwJ9rFrRjru00p40AXk_Q=s64",
      "userId": "04223418218813721824"
     }
    }
   },
   "source": [
    "def normalization(data_array):\n",
    "    \"\"\"\n",
    "    normalize the data array within 0~255\n",
    "    :param data_array: numpy array \n",
    "    :return: data_array: numpy array (overwrite)\n",
    "    \"\"\"\n",
    "    amin = np.amin(data_array)\n",
    "    amax = np.amax(data_array)\n",
    "    scale = 255.0 / (amax - amin)\n",
    "    data_array = data_array - amin\n",
    "    data_array = data_array * scale\n",
    "    data_array = np.uint8(data_array)\n",
    "    return data_array\n",
    "\n",
    "def calc_entropy(tempdata):\n",
    "    \"\"\"\n",
    "    :param tempdata: numpy array\n",
    "    :return: entropy: float32, entropy value (shannon's entropy) of input array\n",
    "    \"\"\"\n",
    "    tempdata = tempdata.values\n",
    "    histgram = [0] * 256\n",
    "    # normalization\n",
    "    tempdata = normalization(tempdata)\n",
    "\n",
    "    for i in range(len(tempdata)):\n",
    "        histgram[tempdata[i]] += 1\n",
    "    entropy = 0\n",
    "    for i in range(256):\n",
    "        p = histgram[i] / len(tempdata)\n",
    "        if p == 0:\n",
    "            continue\n",
    "        entropy -= p * math.log2(p)\n",
    "    return entropy\n",
    "\n",
    "def Entropy_analysis(data, x_axis, flag):\n",
    "    \"\"\"\n",
    "    calculate entropy values of input 2D wave (wavenum*xyz)\n",
    "    make line plot (x: wavenum, y: entropy)\n",
    "    make df \n",
    "\n",
    "    :param data: numpy array\n",
    "    :param x_axis: numpy array \n",
    "    :param flag: tag for analysis, full means full length of data (500-3500)\n",
    "\n",
    "    :return None \n",
    "    \"\"\"\n",
    "\n",
    "    entropy_values = []\n",
    "    for i in range(len(data.T)):\n",
    "        tempdata = data[i]\n",
    "        entropy_values.append(calc_entropy(tempdata))\n",
    "    entropy_df = pd.DataFrame([x_axis, entropy_values])\n",
    "    # draw fig\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.grid(False)\n",
    "    ax.invert_xaxis()\n",
    "    ax.plot(x_axis, entropy_values, color=\"black\")\n",
    "    # save\n",
    "    plt.savefig(\"./results/figures/entropy_plot_{}.png\".format(flag),\n",
    "                transparent=True, bbox_inches=\"tight\", pad_inches=0.1)\n",
    "    entropy_df.to_csv(\"./results/entropy_df_{}.csv\".format(flag))\n",
    "\n",
    "\n",
    "def PCA_analysis(data, x_axis, flag):\n",
    "    # data normalization\n",
    "    norm_data = data.iloc[:, 1:].apply(lambda x: (x - x.mean()) / x.std(), axis=0)\n",
    "    pca = PCA()\n",
    "    pca.fit(norm_data)\n",
    "    feature = pca.transform(norm_data)\n",
    "    data_PCA = pd.DataFrame(feature,\n",
    "                            columns=[\"PC{}\".format(x + 1) for x in range(len(norm_data.columns))])\n",
    "\n",
    "    # make graph for PC scatter\n",
    "    os.makedirs(\"./results/figures\", exist_ok=True)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(feature[:, 0], feature[:, 1], alpha=0.8, color=\"black\")\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.grid(False)\n",
    "    plt.savefig(\"./results/figures/PCA_scatter_{}.png\".format(flag), transparent=True)\n",
    "\n",
    "    # make graph for CDF\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    pd.DataFrame(pca.explained_variance_ratio_, index=[\"PC{}\".format(x + 1) for x in range(len(norm_data.columns))])\n",
    "    plt.gca().get_xaxis().set_major_locator(ticker.MaxNLocator(integer=True))\n",
    "    plt.plot([0] + list(np.cumsum(pca.explained_variance_ratio_)), \"-o\",\n",
    "             color=\"black\")\n",
    "    plt.xlabel(\"Number of principal components\")\n",
    "    plt.ylabel(\"Cumulative contribution rate\")\n",
    "    plt.grid()\n",
    "    plt.xlim(0, 15)\n",
    "    plt.grid(False)\n",
    "    plt.savefig(\"./results/figures/PCA_CDF_{}.png\".format(flag), transparent=True, bbox_inches=\"tight\", pad_inches=0.1)\n",
    "\n",
    "    # clustering and\n",
    "    extracted_df = data_PCA#.iloc[:, 0:cluster_num]\n",
    "    cust_array = extracted_df.to_numpy()\n",
    "    cust_array = cust_array\n",
    "    pred = KMeans(n_clusters=cluster_num).fit_predict(cust_array)\n",
    "    pred_image = np.reshape(pred, [x, y])\n",
    "    plt.imsave('./results/figures/clustered_img_{}.png'.format(flag), pred_image)\n",
    "\n",
    "    # class differentiation\n",
    "    os.makedirs(\"./results/spectrum_{}\".format(flag), exist_ok=True)\n",
    "    os.makedirs(\"./results/figures/Class_images_{}\".format(flag), exist_ok=True)\n",
    "    Class_list = []\n",
    "    for i in range(cluster_num):\n",
    "        Class_list.append(np.where(pred != i, 0, 1))\n",
    "        plt.imsave(\"./results/figures/Class_images_{0}/Class{1}.png\".format(flag, i),\n",
    "                   np.reshape(np.where(pred != i,0,1), [x, y]))\n",
    "\n",
    "    for i in range(cluster_num):\n",
    "        fig, ax = plt.subplots()\n",
    "        Class_data = data.T * Class_list[i]\n",
    "        Class_data = np.mean(Class_data.T.values, axis = 0)\n",
    "        Class_min = np.min(Class_data)\n",
    "        Class_max = np.max(Class_data)\n",
    "        norm_Class_data = (Class_data - Class_min)/(Class_max - Class_min)\n",
    "        ax.set_ylim([-0.2, 1])\n",
    "        ax.grid(False)\n",
    "        ax.invert_xaxis()\n",
    "        ax.plot(x_axis, norm_Class_data, color=\"black\")\n",
    "        plt.savefig(\"./results/spectrum_{0}/norm_Class_{1}.png\".format(flag, i),\n",
    "                    transparent=True, bbox_inches=\"tight\", pad_inches=0.1)\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_ylim([-0.2, 1])\n",
    "        ax.grid(False)\n",
    "        ax.invert_xaxis()\n",
    "        ax.plot(x_axis, Class_data, color=\"black\")\n",
    "        plt.savefig(\"./results/spectrum_{0}/Class_{1}.png\".format(flag, i),\n",
    "                    transparent=True, bbox_inches=\"tight\", pad_inches=0.1)\n",
    "        \n",
    "        pd.DataFrame(Class_data).to_csv(\"./results/spectrum_{0}/Class_{1}.csv\".format(flag,i))\n",
    "\n",
    "def execute_analysis(data_path, axis_path):\n",
    "  os.chdir(os.path.dirname(data_path))\n",
    "  # all range analysis\n",
    "  data = pd.read_csv(data_path, header=None).T[0:x*y]\n",
    "  x_axis = np.loadtxt(axis_path)\n",
    "  PCA_analysis(data, x_axis, flag = \"full\")\n",
    "  #Entropy_analysis(data, x_axis, flag = \"full\")\n",
    "\n",
    "  # subrange analysis \n",
    "  subrange_axis = x_axis[np.where((x_axis>subrange[0])&(x_axis<subrange[1]))]\n",
    "  subrange_data = data.T.iloc[np.where((x_axis>subrange[0])&(x_axis<subrange[1]))].T\n",
    "  PCA_analysis(subrange_data, subrange_axis, flag = \"subrange\")\n",
    "  Entropy_analysis(data, x_axis, flag = \"subrange\")"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MnKoKA-P3Enm"
   },
   "source": [
    "execute_analysis(data_path, axis_path)"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "smWMsn-s9bRF"
   },
   "source": [],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}